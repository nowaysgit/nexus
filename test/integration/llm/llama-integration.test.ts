import { Test, TestingModule } from '@nestjs/testing';
import { ConfigService } from '@nestjs/config';
import { LlamaProviderService } from '../../../src/llm/providers/llama-provider.service';
import { LogService } from '../../../src/logging/log.service';
import {
  LLMProviderType,
  LLMMessageRole,
  ILLMMessage,
} from '../../../src/common/interfaces/llm-provider.interface';

/**
 * Integration —Ç–µ—Å—Ç—ã –¥–ª—è LlamaProviderService —Å —Ä–µ–∞–ª—å–Ω—ã–º –ª–æ–∫–∞–ª—å–Ω—ã–º LLM
 * –¢—Ä–µ–±—É–µ—Ç –∑–∞–ø—É—â–µ–Ω–Ω—ã–π Ollama —Å–µ—Ä–≤–µ—Ä –Ω–∞ localhost:11434 —Å –º–æ–¥–µ–ª—å—é llama3.2
 *
 * –ó–∞–ø—É—Å–∫:
 * 1. docker-compose -f docker-compose.llm.yml up -d
 * 2. yarn test:integration --testNamePattern="LlamaProviderService Integration"
 */
describe('LlamaProviderService Integration Tests', () => {
  let service: LlamaProviderService;
  let logService: LogService;

  // –§–ª–∞–≥ –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤ –µ—Å–ª–∏ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
  let isLLMAvailable = false;

  beforeAll(async () => {
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ LLM –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º —Ç–µ—Å—Ç–æ–≤
    try {
      const response = await fetch('http://localhost:11434/api/tags');
      isLLMAvailable = response.ok;
    } catch (_error) {
      console.warn('‚ö†Ô∏è –õ–æ–∫–∞–ª—å–Ω—ã–π LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã.');
      isLLMAvailable = false;
    }
  });

  beforeEach(async () => {
    const module: TestingModule = await Test.createTestingModule({
      providers: [
        LlamaProviderService,
        {
          provide: ConfigService,
          useValue: {
            get: jest.fn().mockReturnValue({
              endpoint: 'http://localhost:11434', // –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Ollama –ø–æ—Ä—Ç
              apiKey: '',
              model: 'llama3.2',
              timeout: 60000,
            }),
          },
        },
        {
          provide: LogService,
          useValue: {
            setContext: jest.fn(),
            log: jest.fn(),
            debug: jest.fn(),
            warn: jest.fn(),
            error: jest.fn(),
          },
        },
      ],
    }).compile();

    service = module.get<LlamaProviderService>(LlamaProviderService);
    logService = module.get<LogService>(LogService);
  });

  describe('Real LLM Integration', () => {
    it('should check availability of local Llama server', async () => {
      if (!isLLMAvailable) {
        console.log('üèÉ‚Äç‚ôÇÔ∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç - LLM —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω');
        return;
      }

      const isAvailable = await service.checkAvailability();
      expect(isAvailable).toBe(true);
    }, 30000);

    it('should generate real text response', async () => {
      if (!isLLMAvailable) {
        console.log('üèÉ‚Äç‚ôÇÔ∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç - LLM —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω');
        return;
      }

      const messages: ILLMMessage[] = [
        {
          role: LLMMessageRole.USER,
          content: 'Say "Hello from Llama!" in exactly 3 words.',
        },
      ];

      const result = await service.generateText(messages, {
        maxTokens: 10,
        temperature: 0, // –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
      });

      expect(result).toBeDefined();
      expect(result.text).toBeDefined();
      expect(typeof result.text).toBe('string');
      expect(result.text.length).toBeGreaterThan(0);
      expect(result.requestInfo).toBeDefined();
      expect(result.requestInfo?.totalTokens).toBeGreaterThan(0);

      console.log('ü§ñ LLM Response:', result.text);
    }, 45000);

    it('should handle multiple message conversation', async () => {
      if (!isLLMAvailable) {
        console.log('üèÉ‚Äç‚ôÇÔ∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç - LLM —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω');
        return;
      }

      const messages: ILLMMessage[] = [
        {
          role: LLMMessageRole.SYSTEM,
          content: 'You are a helpful assistant. Always respond with exactly one word.',
        },
        {
          role: LLMMessageRole.USER,
          content: 'What is the capital of France?',
        },
      ];

      const result = await service.generateText(messages, {
        maxTokens: 5,
        temperature: 0,
      });

      expect(result).toBeDefined();
      expect(result.text).toBeDefined();
      expect(result.text.toLowerCase()).toContain('paris');

      console.log('üó∫Ô∏è Geography Response:', result.text);
    }, 45000);

    it('should generate JSON response', async () => {
      if (!isLLMAvailable) {
        console.log('üèÉ‚Äç‚ôÇÔ∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç - LLM —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω');
        return;
      }

      const messages: ILLMMessage[] = [
        {
          role: LLMMessageRole.USER,
          content: 'Return a JSON object with key "test" and value "success"',
        },
      ];

      const result = await service.generateJSON(messages, {
        maxTokens: 20,
        temperature: 0,
      });

      expect(result).toBeDefined();
      expect(result.data).toBeDefined();
      expect(typeof result.data).toBe('object');
      expect(result.data).toHaveProperty('test', 'success');

      console.log('üìã JSON Response:', result.data);
    }, 45000);

    it('should handle error responses gracefully', async () => {
      if (!isLLMAvailable) {
        console.log('üèÉ‚Äç‚ôÇÔ∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç - LLM —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω');
        return;
      }

      // –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–≤–∏—Å —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
      const testModule: TestingModule = await Test.createTestingModule({
        providers: [
          LlamaProviderService,
          {
            provide: ConfigService,
            useValue: {
              get: jest.fn().mockReturnValue({
                endpoint: 'http://localhost:99999', // –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø–æ—Ä—Ç
                apiKey: '',
                model: 'non-existent-model',
                timeout: 5000,
              }),
            },
          },
          {
            provide: LogService,
            useValue: logService,
          },
        ],
      }).compile();

      const errorService = testModule.get<LlamaProviderService>(LlamaProviderService);

      const messages: ILLMMessage[] = [
        {
          role: LLMMessageRole.USER,
          content: 'This should fail',
        },
      ];

      await expect(errorService.generateText(messages)).rejects.toThrow();
    }, 30000);

    it('should provide correct provider info', async () => {
      const info = service.getProviderInfo();

      expect(info).toBeDefined();
      expect(info.type).toBe(LLMProviderType.LLAMA);
      expect(info.name).toBe('Llama 3.2');
      expect(info.models.some(model => model.includes('llama3.2'))).toBe(true);
      expect(info.features).toContain('text_generation');
      expect(info.features).toContain('json_generation');
    });

    it('should estimate tokens approximately', () => {
      const text = 'This is a test message for token estimation.';
      const estimatedTokens = service.estimateTokens(text);

      expect(estimatedTokens).toBeGreaterThan(0);
      expect(estimatedTokens).toBeLessThan(50); // –†–∞–∑—É–º–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    });
  });

  describe('Performance Tests', () => {
    it('should respond within acceptable time limits', async () => {
      if (!isLLMAvailable) {
        console.log('üèÉ‚Äç‚ôÇÔ∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç - LLM —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω');
        return;
      }

      const startTime = Date.now();

      const messages: ILLMMessage[] = [
        {
          role: LLMMessageRole.USER,
          content: 'Quick response test',
        },
      ];

      await service.generateText(messages, {
        maxTokens: 5,
        temperature: 0,
      });

      const responseTime = Date.now() - startTime;

      // –û–±–ª–µ–≥—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –æ—Ç–≤–µ—á–∞—Ç—å –±—ã—Å—Ç—Ä–æ (–º–µ–Ω–µ–µ 30 —Å–µ–∫—É–Ω–¥)
      expect(responseTime).toBeLessThan(30000);

      console.log(`‚ö° Response time: ${responseTime}ms`);
    }, 35000);

    it('should handle concurrent requests', async () => {
      if (!isLLMAvailable) {
        console.log('üèÉ‚Äç‚ôÇÔ∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç - LLM —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω');
        return;
      }

      const messages: ILLMMessage[] = [
        {
          role: LLMMessageRole.USER,
          content: 'Concurrent test',
        },
      ];

      const promises = Array.from({ length: 3 }, () =>
        service.generateText(messages, {
          maxTokens: 5,
          temperature: 0,
        }),
      );

      const results = await Promise.all(promises);

      expect(results).toHaveLength(3);
      results.forEach(result => {
        expect(result.text).toBeDefined();
        expect(result.text.length).toBeGreaterThan(0);
      });

      console.log('üî• Concurrent requests completed successfully');
    }, 60000);
  });
});
